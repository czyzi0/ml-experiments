{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = torchtext.data.Field(\n",
    "    sequential=True,\n",
    "    lower=True,\n",
    "    preprocessing=torchtext.data.Pipeline(\n",
    "        lambda x: re.sub(r'[—–’°!\"#$£￥%&\\'()*+,\\-./:;=?@[\\\\\\]^_`{|}~0-9]', '', x)),\n",
    "    pad_token=None,\n",
    "    stop_words=['', '<eos>', '<unk>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 83083869 tokens in 207.0s\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "data_set, = torchtext.datasets.WikiText103.splits(\n",
    "    TEXT, root='../data/wikitext-103', validation=None, test=None)\n",
    "print(f'Loaded {len(data_set[0].text)} tokens in {time.time() - start:.1f}s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(data_set, min_freq=10)#, max_size=60000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created vocab with length: 111297\n",
      " - index of \"queen\": 815\n",
      " - index of \"king\": 210\n",
      " - index of \"2011\": 0\n",
      " - index of \"戦場のヴァルキュリア3\": 0\n",
      " - index of \"<eos>\": 0\n",
      " - index of \"valkyria\": 43903\n",
      " - index of \"chronicles\": 6763\n",
      " - index of \"iii\": 1091\n",
      " - word with index \"0\": <unk>\n",
      " - word with index \"1\": the\n",
      " - word with index \"2\": of\n",
      " - word with index \"15\": is\n"
     ]
    }
   ],
   "source": [
    "print(f'Created vocab with length: {len(TEXT.vocab)}')\n",
    "for word in ['queen', 'king', '2011', '戦場のヴァルキュリア3', '<eos>', 'valkyria', 'chronicles', 'iii']:\n",
    "    print(f' - index of \"{word}\": {TEXT.vocab.stoi[word]}')\n",
    "for i in [0, 1, 2, 15]:\n",
    "    print(f' - word with index \"{i}\": {TEXT.vocab.itos[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['valkyria', 'chronicles', 'iii', 'senjō']\n",
      "torch.Size([4, 1])\n",
      "tensor([[43903],\n",
      "        [ 6763],\n",
      "        [ 1091],\n",
      "        [    0]])\n"
     ]
    }
   ],
   "source": [
    "x1 = data_set[0].text[:4]\n",
    "print(x1)\n",
    "x2 = TEXT.process([x1])\n",
    "print(x2.size())\n",
    "print(x2)\n",
    "# print(x1 == x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkipGramDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data, vocab):\n",
    "        self.data = data\n",
    "        self.vocab = vocab\n",
    "\n",
    "        freqs = Counter()\n",
    "        for word, freq in self.vocab.freqs.items():\n",
    "            freqs.update({self.vocab.stoi[word]: freq})\n",
    "        # Build negatives table\n",
    "        x = np.array([f for _, f in sorted(freqs.items())]) ** 0.75\n",
    "        x = (1e8 * x / x.sum()).round()\n",
    "        x = Counter({i: int(c) for i, c in zip(sorted(freqs.keys()), x)})\n",
    "        self.negatives = np.array(list(x.elements()))\n",
    "        np.random.shuffle(self.negatives)\n",
    "        # Build sampling table\n",
    "        x = np.array([f for _, f in sorted(freqs.items())]) / len(self.data)\n",
    "        x = np.sqrt(0.001 / x) + 0.001 / x\n",
    "        self.discards = {i: p for i, p in zip(sorted(freqs.keys()), x) if p < 1}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83083869"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = SkipGramDataset(data_set[0].text, TEXT.vocab)\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
